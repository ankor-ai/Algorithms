{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Practical Reinforcement Learning MOOC on Coursera](https://www.coursera.org/learn/practical-rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 10:  REINFORCE in TensorFlow\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_actions:  2\n",
      "state_dim:  (4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f54894ae048>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f548dddc438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "#gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env,'env'):\n",
    "    env=env.env\n",
    "\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "print(\"n_actions: \", n_actions)\n",
    "print(\"state_dim: \", state_dim)\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__. \n",
    "\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.get_shape() =  (?, 4)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create input variables. We only need <s,a,R> for REINFORCE\n",
    "states = tf.placeholder('float32', (None,)+state_dim,name=\"states\") # (None,)+state_dim,name=\"states\"\n",
    "actions = tf.placeholder('int32',name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")\n",
    "\n",
    "print(\"states.get_shape() = \", states.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "\n",
    "policy_net = keras.models.Sequential()\n",
    "policy_net.add(L.InputLayer(state_dim))\n",
    "\n",
    "policy_net.add(L.Dense(150))\n",
    "policy_net.add(L.Activation('linear'))\n",
    "policy_net.add(L.Dense(100))\n",
    "policy_net.add(L.Activation('relu'))\n",
    "policy_net.add(L.Dense(50))\n",
    "policy_net.add(L.Activation('linear'))\n",
    "policy_net.add(L.Dense(n_actions))\n",
    "\n",
    "#<linear outputs (symbolic) of your network>\n",
    "logits = policy_net(states) \n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function to pick action in one given state\n",
    "get_action_proba = lambda s: policy.eval({states:[s]})[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get probabilities for parti\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# policy objective as in the last formula. please use mean, not sum.\n",
    "# note: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions * cumulative_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regularize with entropy\n",
    "#<compute entropy. Don't forget the sign!>\n",
    "entropy = -tf.reduce_sum(policy * log_policy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "# get the list of all trainable weights in your network\n",
    "all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "# weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "loss = -J -0.1 * entropy\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    rewards = np.array(rewards)\n",
    "    cumulative_rewards = np.zeros(len(rewards))\n",
    "    for t, reward in enumerate(rewards[::-1]):\n",
    "        tmp_reward = rewards[len(rewards)-t-1:]\n",
    "        tmp_gamma = np.fromiter((gamma**t for t in range(len(tmp_reward))), float)\n",
    "        cumulative_rewards[len(rewards)-t-1] = sum(tmp_reward * tmp_gamma)\n",
    "\n",
    "    return cumulative_rewards #<array of cumulative rewards>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) != 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(_states,_actions,_rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "        \n",
    "        # pick random action using action_probas\n",
    "        a = np.random.choice(n_actions, p=action_probas)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    train_step(states,actions,rewards)\n",
    "            \n",
    "    return sum(rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:83.170\n",
      "mean reward:134.320\n",
      "mean reward:99.980\n",
      "mean reward:108.760\n",
      "mean reward:109.170\n",
      "mean reward:232.260\n",
      "mean reward:18.370\n",
      "mean reward:44.940\n",
      "mean reward:114.250\n",
      "mean reward:309.810\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
